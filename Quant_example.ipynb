{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TinySAM import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm.auto import tqdm, trange\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Quantization Modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NF4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "GroundingModel = GDino()\n",
    "SAMModel_EfficientSAM = EfficientSAM()\n",
    "SAMModel_EfficientViTSAM = EfficientViTSAM()\n",
    "SAMModel_MobileSAM = MobileSAM()\n",
    "SAMModel_SAMHQ = SAMHQ()\n",
    "SAMModel_SAM2 = SAM2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Size Details:\n",
      "Total Parameters: 232,313,216\n",
      "\n",
      "Size by data type (MB):\n",
      "float32: 886.20 MB\n",
      "\n",
      "Original GDino size:\n",
      "Total Size: 886.20 MB\n",
      "--------------------------------------\n",
      "\n",
      "Quantizing GDino model...\n",
      "Quantize percentage: 100.\n",
      "Found 421 Linear layers, will quantize 421 layers\n",
      "End Quantization\n",
      "--------------------------------------\n",
      "\n",
      "Model Size Details:\n",
      "Total Parameters: 129,912,192\n",
      "\n",
      "Size by data type (MB):\n",
      "4bit: 48.83 MB\n",
      "float32: 104.95 MB\n",
      "\n",
      "Quantized GDino size:\n",
      "Total Size: 153.78 MB\n",
      "Size reduction: 82.65%\n"
     ]
    }
   ],
   "source": [
    "model_quantization(GroundingModel, quant_type=\"nf4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Size Details:\n",
      "Total Parameters: 642,686,000\n",
      "\n",
      "Size by data type (MB):\n",
      "float32: 2451.65 MB\n",
      "\n",
      "Original SAMHQ size:\n",
      "Total Size: 2451.65 MB\n",
      "--------------------------------------\n",
      "\n",
      "Quantizing SAMHQ model...\n",
      "Quantize percentage: 100.\n",
      "Found 178 Linear layers, will quantize 178 layers\n",
      "End Quantization\n",
      "--------------------------------------\n",
      "\n",
      "Model Size Details:\n",
      "Total Parameters: 326,060,592\n",
      "\n",
      "Size by data type (MB):\n",
      "4bit: 150.98 MB\n",
      "float32: 35.99 MB\n",
      "\n",
      "Quantized SAMHQ size:\n",
      "Total Size: 186.97 MB\n",
      "Size reduction: 92.37%\n"
     ]
    }
   ],
   "source": [
    "model_quantization(SAMModel_SAMHQ, quant_type=\"nf4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FP4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Size Details:\n",
      "Total Parameters: 224,430,130\n",
      "\n",
      "Size by data type (MB):\n",
      "float32: 856.13 MB\n",
      "\n",
      "Original SAM2 size:\n",
      "Total Size: 856.13 MB\n",
      "--------------------------------------\n",
      "\n",
      "Quantizing SAM2  model...\n",
      "Quantize percentage: 100\n",
      "Found 292 Linear layers, will quantize 292 layers\n",
      "End Quantization\n",
      "--------------------------------------\n",
      "\n",
      "Model Size Details:\n",
      "Total Parameters: 112,931,762\n",
      "\n",
      "Size by data type (MB):\n",
      "4bit: 53.17 MB\n",
      "float32: 5.47 MB\n",
      "\n",
      "Quantized SAM2 size:\n",
      "Total Size: 58.63 MB\n",
      "Size reduction: 93.15%\n"
     ]
    }
   ],
   "source": [
    "model_quantization(SAMModel_SAM2, quant_type=\"fp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Size Details:\n",
      "Total Parameters: 10,130,092\n",
      "\n",
      "Size by data type (MB):\n",
      "float32: 38.64 MB\n",
      "\n",
      "Original MobileSAM size:\n",
      "Total Size: 38.64 MB\n",
      "--------------------------------------\n",
      "\n",
      "Quantizing MobileSAM model...\n",
      "Quantize percentage: 100.\n",
      "Found 88 Linear layers, will quantize 88 layers\n",
      "End Quantization\n",
      "--------------------------------------\n",
      "\n",
      "Model Size Details:\n",
      "Total Parameters: 5,640,108\n",
      "\n",
      "Size by data type (MB):\n",
      "4bit: 2.14 MB\n",
      "float32: 4.39 MB\n",
      "\n",
      "Quantized MobileSAM size:\n",
      "Total Size: 6.53 MB\n",
      "Size reduction: 83.11%\n"
     ]
    }
   ],
   "source": [
    "model_quantization(SAMModel_MobileSAM, quant_type=\"fp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Demo9)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
